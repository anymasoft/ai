"""
JobRadar v0 - –î–≤–∏–∂–æ–∫ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π (include/require/exclude)
"""
import logging
from sqlalchemy.orm import Session
from models import FilterRule, FilterTerm

logger = logging.getLogger(__name__)


def init_keyword_filter(db: Session) -> None:
    """
    –°–æ–∑–¥–∞—ë—Ç Keywords –ø—Ä–∞–≤–∏–ª–æ (OR —Ä–µ–∂–∏–º) –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ, –µ—Å–ª–∏ filter_rules –ø—É—Å—Ç–∞
    –¢–∞–∫–∂–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ —Å "Legacy keywords" –Ω–∞ "Keywords"

    –°–æ–∑–¥–∞—ë—Ç –æ–¥–Ω—É –∑–∞–ø–∏—Å—å:
    - name="Keywords"
    - mode="keyword_or"
    - enabled=True

    –ë–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–µ—Ä–º–æ–≤
    """
    existing_rules = db.query(FilterRule).first()
    if not existing_rules:
        keyword_rule = FilterRule(
            name="Keywords",
            mode="keyword_or",
            enabled=True
        )
        db.add(keyword_rule)
        db.commit()
        logger.info("‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∞–≤–∏–ª–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ Keywords (OR —Ä–µ–∂–∏–º)")
    else:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ —Å "Legacy keywords" –Ω–∞ "Keywords"
        old_rules = db.query(FilterRule).filter(FilterRule.name == "Legacy keywords").all()
        for rule in old_rules:
            rule.name = "Keywords"

        # –û–±–Ω–æ–≤–ª—è–µ–º mode —Å "legacy_or" –Ω–∞ "keyword_or"
        legacy_mode_rules = db.query(FilterRule).filter(FilterRule.mode == "legacy_or").all()
        for rule in legacy_mode_rules:
            rule.mode = "keyword_or"

        if old_rules or legacy_mode_rules:
            db.commit()
            logger.info(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö –ø—Ä–∞–≤–∏–ª: name={len(old_rules)}, mode={len(legacy_mode_rules)}")


def normalize_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç: lowercase + collapse spaces
    """
    if not text:
        return ""
    return " ".join(text.lower().split())


def load_active_filter(db: Session) -> dict:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏–∑ –ë–î

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict:
    {
        "mode": "keyword_or" –∏–ª–∏ "advanced",
        "include_any": [],
        "require_all": [],
        "exclude_any": []
    }

    –ï—Å–ª–∏ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞ –Ω–µ—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç mode="keyword_or"
    """
    active_rule = db.query(FilterRule).filter(FilterRule.enabled == True).first()

    result = {
        "mode": "keyword_or",
        "include_any": [],
        "require_all": [],
        "exclude_any": []
    }

    if not active_rule:
        return result

    result["mode"] = active_rule.mode

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã —ç—Ç–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞
    terms = db.query(FilterTerm).filter(
        FilterTerm.rule_id == active_rule.id,
        FilterTerm.enabled == True
    ).all()

    for term in terms:
        normalized_value = normalize_text(term.value)
        if term.term_type == "include":
            result["include_any"].append(normalized_value)
        elif term.term_type == "require":
            result["require_all"].append(normalized_value)
        elif term.term_type == "exclude":
            result["exclude_any"].append(normalized_value)

    logger.debug(
        f"Filter result: mode={result['mode']}, "
        f"include={len(result['include_any'])}, "
        f"require={len(result['require_all'])}, "
        f"exclude={len(result['exclude_any'])}"
    )

    return result


def match_text(text: str, filter_config: dict, legacy_keywords: list) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–∞–≤–∏–ª–∞–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏

    Args:
        text: –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
        filter_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞ –∏–∑ load_active_filter()
        legacy_keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã Keyword (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä)

    Returns:
        True –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ, –∏–Ω–∞—á–µ False
    """
    normalized_text = normalize_text(text)
    mode = filter_config.get("mode", "keyword_or")

    if mode == "keyword_or":
        # –†–µ–∂–∏–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (OR): –ø—É–±–ª–∏–∫—É–µ–º –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
        return any(kw.lower() in normalized_text for kw in legacy_keywords)

    # –†–µ–∂–∏–º "advanced"
    exclude_any = filter_config.get("exclude_any", [])
    require_all = filter_config.get("require_all", [])
    include_any = filter_config.get("include_any", [])

    logger.debug(f"üìä Match check - exclude={exclude_any}, require={require_all}, include={include_any}")

    # 1. –ï—Å–ª–∏ –µ—Å—Ç—å –∏—Å–∫–ª—é—á–∞–µ–º–æ–µ —Å–ª–æ–≤–æ - –Ω–µ –ø—É–±–ª–∏–∫—É–µ–º
    if any(exc in normalized_text for exc in exclude_any):
        logger.debug(f"‚ùå Found exclude word in text")
        return False

    # 2. –ï—Å–ª–∏ –µ—Å—Ç—å —Ç—Ä–µ–±—É–µ–º—ã–µ —Å–ª–æ–≤–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
    if require_all and not all(req in normalized_text for req in require_all):
        logger.debug(f"‚ùå Not all require words found. require={require_all}, text_sample={normalized_text[:100]}")
        return False

    # 3. –ï—Å–ª–∏ –Ω–µ—Ç include —Å–ª–æ–≤ - –ø—É–±–ª–∏–∫—É–µ–º (–±—ã–ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è)
    if not include_any:
        logger.debug(f"‚úÖ No include words - publishing")
        return True

    # 4. –ï—Å–ª–∏ –µ—Å—Ç—å include —Å–ª–æ–≤–∞ - –ø—É–±–ª–∏–∫—É–µ–º –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –µ—Å—Ç—å
    result = any(inc in normalized_text for inc in include_any)
    logger.debug(f"{'‚úÖ' if result else '‚ùå'} Include check result={result}")
    return result
